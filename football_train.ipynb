{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCV38q_u7cGO"
      },
      "source": [
        "*Task*: For each soccer player, return cropped images of 10 players and the jersey numbers of those 10 players"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k1DEVgR07cGU"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "import shutil\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torch.optim import SGD, Adagrad, Adam\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import pickle\n",
        "import numpy as np\n",
        "# from google.colab.patches import cv2_imshow\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from tqdm.autonotebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p-1snijn7cGW"
      },
      "outputs": [],
      "source": [
        "class FootballDataset(Dataset):\n",
        "    def __init__(self, root, transform = None):\n",
        "        self.images = []\n",
        "        self.labels = []\n",
        "        self.file_names = []\n",
        "        self.num_frames = []\n",
        "\n",
        "        matches = os.listdir(root)\n",
        "        for match in matches:\n",
        "            folder_path = os.path.join(root, match)\n",
        "            json_path, video_path = sorted(os.listdir(folder_path))\n",
        "            self.file_names.append(os.path.join(folder_path, json_path.replace(\".json\", \"\")))\n",
        "            with open(os.path.join(folder_path, json_path), \"r\") as json_file:\n",
        "                json_data = json.load(json_file)\n",
        "\n",
        "            # count number of frame\n",
        "            self.num_frames.append(len(json_data[\"images\"]))\n",
        "\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        # Returns the total number of frames\n",
        "        return sum(self.num_frames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # index belongs to video\n",
        "        if index < self.num_frames[0]:\n",
        "            frame_id = index\n",
        "            video_id = 0\n",
        "        elif self.num_frames[0] <= index < self.num_frames[0] + self.num_frames[1]:\n",
        "            frame_id = index - self.num_frames[0]\n",
        "            video_id = 1\n",
        "        else:\n",
        "            frame_id = index - self.num_frames[0] - self.num_frames[1]\n",
        "            video_id = 2\n",
        "\n",
        "        video_path = \"{}.mp4\".format(self.file_names[video_id])\n",
        "        json_path = \"{}.json\".format(self.file_names[video_id])\n",
        "\n",
        "        # Read video\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
        "        flag, image = cap.read()\n",
        "        # cv2.imwrite(\"sample.jpg\", image)\n",
        "\n",
        "        with open(json_path, \"r\") as json_file:\n",
        "            json_data = json.load(json_file)\n",
        "        # print(json_data[\"annotations\"])\n",
        "        bboxes = [anno[\"bbox\"] for anno in json_data[\"annotations\"] \\\n",
        "                              if anno[\"image_id\"] - 1 == frame_id \\\n",
        "                                and anno[\"category_id\"] == 4] # category_id is human\n",
        "        jerseys = [int(anno[\"attributes\"][\"jersey_number\"]) for anno in json_data[\"annotations\"] \\\n",
        "                              if anno[\"image_id\"] -1 == frame_id \\\n",
        "                                and anno[\"category_id\"] == 4]\n",
        "        colors = [anno[\"attributes\"][\"team_jersey_color\"] for anno in json_data[\"annotations\"] \\\n",
        "                              if anno[\"image_id\"] -1 == frame_id \\\n",
        "                                and anno[\"category_id\"] == 4]\n",
        "        colors = [0 if color == \"black\" else 1 for color in colors]\n",
        "        cropped_images = [image[int(y_min): int(y_min + height), int(x_min): int(x_min + width), :] \\\n",
        "                       for (x_min, y_min, width, height) in bboxes]\n",
        "\n",
        "        # for i, cropped_image in enumerate(cropped_images):\n",
        "        #     cv2.imwrite(\"{}.jpg\".format(i), cropped_image)\n",
        "\n",
        "        if self.transform:\n",
        "            cropped_images = [self.transform(image) for image in cropped_images]\n",
        "        # visualize\n",
        "        # for ann in current_annotation:\n",
        "        #     x_min, y_min, width, height = ann\n",
        "        #     x_min = int(x_min)\n",
        "        #     y_min = int(y_min)\n",
        "        #     x_max = int(x_min + width)\n",
        "        #     y_max = int(y_min + height)\n",
        "        #     cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 0, 255), 2)\n",
        "        # cv2.imwrite(\"sample.jpg\", image)\n",
        "        return cropped_images, jerseys, colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "I1Pe3Kby2gc6"
      },
      "outputs": [],
      "source": [
        "def get_args():\n",
        "    parser = argparse.ArgumentParser(description='Football')\n",
        "    parser.add_argument('-p', '--data_path_train', type=str, default=\"./Data/football_train\")\n",
        "    parser.add_argument('-a', '--data_path_test', type=str, default=\"./Data/football_test\")\n",
        "    parser.add_argument('-b', '--batch_size', type=int, default=4)\n",
        "    parser.add_argument('-e', '--epochs', type=int, default=10)\n",
        "    parser.add_argument('-l', '--lr', type=float, default=1e-3)  # SGD: lr = 1e-2. Adam: lr = 1e-3\n",
        "    parser.add_argument('-s', '--image_size', type=int, default=224)\n",
        "    parser.add_argument('-c', '--checkpoint_path', type=str, default=None)\n",
        "    parser.add_argument('-t', '--tensorboard_path', type=str, default=\"tensorboard\")\n",
        "    parser.add_argument('-r', '--trained_path', type=str, default=\"trained_models\")\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    return args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Bdhno89u7cGY"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    images, labels, colors = zip(*batch)\n",
        "\n",
        "    final_images = []\n",
        "    for image in images:\n",
        "        final_images.extend(image)\n",
        "    final_images = torch.stack(final_images)\n",
        "\n",
        "    final_labels = []\n",
        "    for label in labels:\n",
        "        final_labels.extend(label)\n",
        "    final_labels = torch.LongTensor(final_labels)\n",
        "\n",
        "    final_colors = []\n",
        "    for color in colors:\n",
        "        final_colors.extend(color)\n",
        "    final_colors = torch.LongTensor(final_colors)\n",
        "\n",
        "    return final_images, final_labels, final_colors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IAeDZ-YmRBYz"
      },
      "outputs": [],
      "source": [
        "class ResNet_two_header2(nn.Module):\n",
        "    def __init__(self, num_jerseys = 20, num_colors = 2):\n",
        "        super().__init__()\n",
        "        self.model = models.resnet50(pretrained = True)\n",
        "        self.model.fc1 = nn.Linear(in_features = 2048, out_features = num_jerseys)\n",
        "        self.model.fc2 = nn.Linear(in_features = 2048, out_features = num_colors)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model.conv1(x)\n",
        "        x = self.model.bn1(x)\n",
        "        x = self.model.relu(x)\n",
        "        x = self.model.maxpool(x)\n",
        "\n",
        "        x = self.model.layer1(x)\n",
        "        x = self.model.layer2(x)\n",
        "        x = self.model.layer3(x)\n",
        "        x = self.model.layer4(x)\n",
        "\n",
        "        x = self.model.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x1 = self.model.fc1(x)\n",
        "        x2 = self.model.fc2(x)\n",
        "\n",
        "        return x1, x2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "r7ussy6wGYTM"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(writer, cm, class_names, epoch):\n",
        "    \"\"\"\n",
        "    Returns a matplotlib figure containing the plotted confusion matrix.\n",
        "\n",
        "    Args:\n",
        "       cm (array, shape = [n, n]): a confusion matrix of integer classes\n",
        "       class_names (array, shape = [n]): String names of the integer classes\n",
        "    \"\"\"\n",
        "\n",
        "    figure = plt.figure(figsize=(20, 20))\n",
        "    # color map: https://matplotlib.org/stable/gallery/color/colormap_reference.html\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=\"cool\")\n",
        "    plt.title(\"Confusion matrix\")\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Normalize the confusion matrix.\n",
        "    cm = np.around(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], decimals=2)\n",
        "\n",
        "    # Use white text if squares are dark; otherwise black.\n",
        "    threshold = cm.max() / 2.\n",
        "\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            color = \"white\" if cm[i, j] > threshold else \"black\"\n",
        "            plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=color)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    writer.add_figure('confusion_matrix', figure, epoch)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "h-rXA0ah1aKu"
      },
      "outputs": [],
      "source": [
        "def train(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    transform = Compose([\n",
        "        ToTensor(),\n",
        "        Resize((args.image_size, args.image_size))\n",
        "    ])\n",
        "\n",
        "    train_set = FootballDataset(root=args.data_path_train, transform=transform)\n",
        "    valid_set = FootballDataset(root=args.data_path_test, transform=transform)\n",
        "\n",
        "    training_params = {\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"shuffle\": True,\n",
        "        \"drop_last\": True,\n",
        "        # \"num_workers\": 6,\n",
        "        \"collate_fn\": collate_fn\n",
        "    }\n",
        "\n",
        "    valid_params = {\n",
        "        \"batch_size\": args.batch_size,\n",
        "        \"shuffle\": False,\n",
        "        \"drop_last\": False,\n",
        "        # \"num_workers\": 6,\n",
        "        \"collate_fn\": collate_fn\n",
        "    }\n",
        "\n",
        "    train_dataloader = DataLoader(train_set, **training_params)\n",
        "    valid_dataloader = DataLoader(valid_set, **valid_params)\n",
        "\n",
        "    model = ResNet_two_header2(20, 2).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=args.lr)\n",
        "    scheduler = MultiStepLR(optimizer, milestones=[30, 60, 90], gamma=0.1)\n",
        "\n",
        "    if args.checkpoint_path and os.path.isfile(args.checkpoint_path):\n",
        "        checkpoint = torch.load(args.checkpoint_path)\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "        start_epoch = checkpoint[\"epoch\"] + 1\n",
        "        best_acc = checkpoint[\"best_acc\"]\n",
        "    else:\n",
        "        start_epoch = 0\n",
        "        best_acc = 0\n",
        "\n",
        "    if os.path.isdir(args.tensorboard_path):\n",
        "        shutil.rmtree(args.tensorboard_path)\n",
        "    os.mkdir(args.tensorboard_path)\n",
        "\n",
        "    if not os.path.isdir(args.trained_path):\n",
        "        os.mkdir(args.trained_path)\n",
        "    writer = SummaryWriter(args.tensorboard_path)\n",
        "    num_iters = len(train_dataloader)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs):\n",
        "        # TRAIN\n",
        "        model.train()\n",
        "        losses = []\n",
        "        progress_bar = tqdm(train_dataloader, colour=\"yellow\")\n",
        "        for iter, (cropped_images, jerseys, colors) in enumerate(progress_bar):\n",
        "            # Move tensor to configured device:\n",
        "            cropped_images = cropped_images.to(device)\n",
        "            jerseys = jerseys.to(device)\n",
        "            colors = colors.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            predictions_jerseys, predictions_colors = model(cropped_images)\n",
        "            loss_jerseys = criterion(predictions_jerseys, jerseys)\n",
        "            loss_colors = criterion(predictions_colors, colors)\n",
        "            loss = loss_jerseys + loss_colors\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            loss_value = loss.item()\n",
        "            progress_bar.set_description(\"Epoch {}/{}. Loss value: {:.4f}\".format(epoch + 1, args.epochs, loss_value))\n",
        "            losses.append(loss_value)\n",
        "            writer.add_scalar(\"Train/Loss\", np.mean(losses), epoch*num_iters+iter)\n",
        "\n",
        "        # VALIDATE\n",
        "        model.eval()\n",
        "        losses = []\n",
        "        all_predictions_jerseys = []\n",
        "        all_predictions_colors = []\n",
        "        all_gts_jerseys = []\n",
        "        all_gts_colors = []\n",
        "        with torch.no_grad():\n",
        "            for iter, (images, jerseys, colors) in enumerate(valid_dataloader):\n",
        "                # Move tensor to configured device:\n",
        "                images = images.to(device)\n",
        "                jerseys = jerseys.to(device)\n",
        "                colors = colors.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                predictions_jerseys, predictions_colors = model(images)\n",
        "                loss_jerseys = criterion(predictions_jerseys, jerseys)\n",
        "                loss_colors = criterion(predictions_colors, colors)\n",
        "                loss = loss_jerseys + loss_colors\n",
        "                losses.append(loss.item())\n",
        "\n",
        "                max_idx_jerseys = torch.argmax(predictions_jerseys, 1)\n",
        "                max_idx_colors = torch.argmax(predictions_colors, 1)\n",
        "\n",
        "                all_gts_jerseys.extend(jerseys.tolist())\n",
        "                all_gts_colors.extend(colors.tolist())\n",
        "                all_predictions_jerseys.extend(max_idx_jerseys.tolist())\n",
        "                all_predictions_colors.extend(max_idx_colors.tolist())\n",
        "\n",
        "        writer.add_scalar(\"Val/Loss\", np.mean(losses), epoch)\n",
        "        acc_jerseys = accuracy_score(all_gts_jerseys, all_predictions_jerseys)\n",
        "        acc_colors = accuracy_score(all_gts_colors, all_predictions_colors)\n",
        "        avg_acc = (acc_jerseys + acc_colors) / 2\n",
        "        writer.add_scalar(\"Val/Accuracy\", avg_acc, epoch)\n",
        "        # conf_matrix_jerseys = confusion_matrix(all_gts_jerseys, all_predictions_jerseys)\n",
        "        # conf_matrix_colors = confusion_matrix(all_gts_colors, all_predictions_colors)\n",
        "        # plot_confusion_matrix(writer, conf_matrix_jerseys, [i for i in range(10)], epoch, \"Jerseys\")\n",
        "        # plot_confusion_matrix(writer, conf_matrix_colors, [i for i in range(2)], epoch, \"Colors\")\n",
        "\n",
        "        checkpoint = {\n",
        "            \"model\": model.state_dict(),\n",
        "            \"optimizer\": optimizer.state_dict(),\n",
        "            \"epoch\": epoch,\n",
        "            \"best_acc\": best_acc,\n",
        "            \"batch_size\": args.batch_size\n",
        "        }\n",
        "\n",
        "        torch.save(checkpoint, os.path.join(args.trained_path, \"last.pt\"))\n",
        "        if avg_acc > best_acc:\n",
        "            torch.save(checkpoint, os.path.join(args.trained_path, \"best.pt\"))\n",
        "            best_acc = avg_acc\n",
        "        scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "rfHXIYIVFi5u",
        "outputId": "74e96b16-8a72-458e-f6e9-5285f33bd978"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "  0%|\u001b[33m          \u001b[0m| 0/1131 [00:00<?, ?it/s]c:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "Epoch 1/10. Loss value: 0.0308: 100%|\u001b[33m██████████\u001b[0m| 1131/1131 [5:59:23<00:00, 19.07s/it]  \n",
            "  0%|\u001b[33m          \u001b[0m| 0/1131 [00:00<?, ?it/s]c:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "Epoch 2/10. Loss value: 0.0654: 100%|\u001b[33m██████████\u001b[0m| 1131/1131 [10:53:22<00:00, 34.66s/it]    \n",
            "  0%|\u001b[33m          \u001b[0m| 0/1131 [00:00<?, ?it/s]c:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "Epoch 3/10. Loss value: 0.0850: 100%|\u001b[33m██████████\u001b[0m| 1131/1131 [6:52:00<00:00, 21.86s/it]   \n",
            "  0%|\u001b[33m          \u001b[0m| 0/1131 [00:00<?, ?it/s]c:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "Epoch 4/10. Loss value: 0.0249: 100%|\u001b[33m██████████\u001b[0m| 1131/1131 [12:25:09<00:00, 39.53s/it]    \n",
            "  0%|\u001b[33m          \u001b[0m| 0/1131 [00:00<?, ?it/s]c:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n",
            "Epoch 5/10. Loss value: 0.0020:   2%|\u001b[33m▏         \u001b[0m| 25/1131 [10:30<8:01:01, 26.10s/it]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x0000024431A21310>>\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Minh Quan\\AppData\\Roaming\\Python\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
            "    def _clean_thread_parent_frames(\n",
            "\n",
            "KeyboardInterrupt: \n",
            "Epoch 5/10. Loss value: 0.0020:   2%|\u001b[33m▏         \u001b[0m| 25/1131 [10:53<8:01:53, 26.14s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     args \u001b[38;5;241m=\u001b[39m get_args()\n\u001b[1;32m----> 3\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[10], line 75\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     74\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 75\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     77\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[1;32mc:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Minh Quan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    args = get_args()\n",
        "    train(args)\n",
        "    # Error"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
